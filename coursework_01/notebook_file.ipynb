 1/1: import pandas as pd
 2/1: import pandas as pd
 2/2:
# load the dataset
df_source = pd.read_csv('data/raw/product-cat-dataset.csv')
df_source.info
 2/3:
# load the dataset
df_source = pd.read_csv('data/raw/product-cat-dataset.csv')
df_source.info()
 2/4:
# load the dataset
df_source = pd.read_csv('data/raw/product-cat-dataset.csv')
df_source.info()
 2/5: df_source.head()
 2/6: df_source.head()
 2/7: df_source.groupby(['Level_ 1', 'Level_2'])
 2/8: df_source.groupby(['Level_1', 'Level_2'])
 2/9: df_source.groupby(['Level_1', 'Level_2']).size()
2/10: df_source.groupby(['Level_1', 'Level_2', 'Level_3']]).size()
2/11: df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size()
2/12: print(df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size())
2/13: df_source.groupby(['Level_1', 'Level_2', 'Level_3']).head()
2/14: df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size()
2/15: pd.DataFrame(df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size())
2/16: pd.DataFrame(df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size(), columns=['Levels'])
2/17:
import pandas as pd
import plotly.express as px
2/18:
import pandas as pd
import plotly.express as px
2/19: fig = px.treemap(df_source.groupby(['Level_1', 'Level_2', 'Level_3']))
2/20:
fig = px.treemap(df_source.groupby(['Level_1', 'Level_2', 'Level_3']))
fig.show()
2/21:
fig = px.treemap(pd.DataFrame(df_source.groupby(['Level_1', 'Level_2', 'Level_3'])))
fig.show()
2/22:
fig = px.treemap(df_source)
fig.show()
2/23:
fig = px.treemap(df_source, path=['Level_1', 'Level_2', 'Level_3'])
fig.show()
2/24: df_grouped = pd.DataFrame(df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size(), columns=['Levels'])
2/25:
df_grouped = pd.DataFrame(df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size(), columns=['Levels'])
df_grouped.head()
2/26:
fig = px.treemap(df_grouped, path=['Level_1', 'Level_2', 'Level_3'])
fig.show()
2/27:
fig = px.treemap(df_grouped, path=['Levels'])
fig.show()
2/28:
fig = px.treemap(df_grouped.unstack(), path=['Level_1', 'Level_2', 'Level_3'])
fig.show()
2/29: source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size()
2/30:
fig = px.treemap(source_grouped.unstack(), path=['Level_1', 'Level_2', 'Level_3'])
fig.show()
2/31: source_grouped.unstack()
2/32:
df_grouped = pd.DataFrame(df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size(), columns=['Levels'])
df_grouped.head()
2/33: df_grouped.unstack()
2/34:
df_grouped = pd.DataFrame(df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size(), columns=['Levels'])
df_grouped.head()
2/35:
df_grouped = pd.DataFrame(df_source.groupby(['Level_1', 'Level_2', 'Level_3']).size(), columns=['Levels'])
df_grouped.head()
2/36: df_grouped.unstack()
2/37: df_grouped.to_flat_index()
2/38: df_grouped.T.reset_index(drop=True).T
2/39: df_grouped.T.reset_index(drop=True)
2/40: df_grouped.T
2/41: df_grouped.T.T
2/42: pd.DataFrame(df_source.groupby(['Level_1', 'Level_2', 'Level_3']).count()
2/43: df_source.groupby(['Level_1', 'Level_2', 'Level_3']).count()
2/44: df_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3']).count()
2/45: df_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3']).count()
2/46:
df_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3']).count()
df_grouped
2/47:
df_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3']).count()
df_grouped.head()
2/48:
fig = px.treemap(df_grouped.unstack(), path=['Level_1', 'Level_2', 'Level_3'])
fig.show()
2/49:
df_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3']).count()
df_grouped.head()
2/50:
fig = px.treemap(df_grouped.unstack(), path=['Level_1', 'Level_2', 'Level_3'])
fig.show()
2/51:
fig = px.treemap(df_grouped, path=['Level_1', 'Level_2', 'Level_3'])
fig.show()
2/52:
df_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3']).count()
df_grouped.head()
2/53: df_grouped.unstack()
2/54: df_grouped.unstack().unstack().unstack()
2/55: df_grouped.unstack().unstack()
2/56: df_grouped
2/57: df_grouped.to_flat_index()
2/58: df_grouped.to_frame(name = 'count').reset_index()
2/59: source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
2/60: source_grouped.size().to_frame(name = 'count').reset_index()
2/61: source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
2/62:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped.count().head()
2/63:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped.count().head()
2/64:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped.to_frame(name = 'count')
2/65:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped.size()to_frame(name = 'count')
2/66:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped.size().to_frame(name = 'count')
2/67: source_grouped.size().to_frame(name = 'count').reset_index()
2/68:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped.size().to_frame(name = 'count').head()
2/69: source_grouped.size().to_frame(name = 'count')
2/70: source_grouped.size().to_frame(name = 'count').reset_index()
2/71: source_grouped.size().to_frame(name = 'count').reset_index()
2/72: source_grouped.count()
2/73: source_grouped.count().reset_index()
2/74:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped.count()
2/75:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped.count().head()
2/76: source_grouped.count().reset_index()
2/77:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped.size().to_frame(name = 'count').head()
2/78: source_grouped.size().to_frame(name = 'count').reset_index()
2/79: source_grouped.size().to_frame(name = 'count').reset_index().head()
2/80: source_grouped.size().to_frame(name = 'count').reset_index()
2/81:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
2/82:
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head()
2/83: source_grouped_count.reset_index().head()
2/84:
fig = px.treemap(source_grouped_count, path=['Level_1', 'Level_2', 'Level_3'])
fig.show()
2/85:
fig = px.treemap(source_grouped_count, path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.show()
2/86:
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.show()
2/87:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head()
2/88:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.show()
2/89:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
2/90:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(20)
2/91:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
2/92:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=50, l=25, r=25, b=25)
fig.show()
2/93:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')

fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))
fig.show()
2/94:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
2/95:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count', root_color="lightblue")
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
2/96:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
2/97: df_source.head()
2/98:
# load the dataset
df_source = pd.read_csv('data/raw/product-cat-dataset.csv')
df_source.info()
2/99: df_source.head()
2/100:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
2/101:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
2/102:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
df_source = pd.read_csv(source_path)
df_source.info()
2/103:
# load the dataset
#source_path = 'data/raw/product-cat-dataset.csv'
source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
2/104: df_source.head()
2/105:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
2/106:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
 3/1:
import pandas as pd
import plotly.express as px
 3/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
 3/3: df_source.head()
 3/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
 3/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
 3/6: # Check if data has missing values in the Description column
 3/7: # Deal with missing values
 3/8: # Apply to Level_1
 3/9: # Apply to Level_2
3/10: # Apply to Level_3
3/11:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
3/12:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
3/13:
# determine the number of categories
df_source[['Level_1']].distinct()
3/14:
# determine the number of categories
df_source[['Level_1']]
3/15:
# determine the number of categories
df_source[['Level_1']].unique()
3/16:
# determine the number of categories
df_source[['Level_1']].unique
3/17:
# determine the number of categories
df_source[['Level_1']].nunique()
3/18:
# determine the number of categories
df_source[['Level_1']].nunique()[1]
3/19:
# determine the number of categories
df_source[['Level_1']].nunique()
3/20:
# determine the number of categories
df_source[['Level_1']].drop_duplicates()
3/21:
# determine the number of categories
df_source[['Level_1']].drop_duplicates().shape[0]
3/22:
# determine the number of categories
print('Distict Level 1 Categories:%' % df_source[['Level_1']].drop_duplicates().shape[0])
3/23:
# determine the number of categories
print('Distict Level 1 Categories:%f' % df_source[['Level_1']].drop_duplicates().shape[0])
3/24:
# determine the number of categories
print('Distict Level 1 Categories:%d' % df_source[['Level_1']].drop_duplicates().shape[0])
3/25:
# determine the number of categories
print('Distict Level 1 Categories: %d' % df_source[['Level_1']].drop_duplicates().shape[0])
3/26:
# determine the number of distinct
print('---Distinct Categories')
print('Level 1: %d' % df_source[['Level_1']].drop_duplicates().shape[0])
3/27:
# determine the number of distinct
print('--- Distinct Categories')
print('Level 1: %d' % df_source[['Level_1']].drop_duplicates().shape[0])
3/28:
# determine the number of distinct
print('--- Distinct Categories')
print('Level 1: %d' % df_source[['Level_1']].drop_duplicates().shape[0])
print('Level 2: %d' % df_source[['Level_2']].drop_duplicates().shape[0])
print('Level 3: %d' % df_source[['Level_3']].drop_duplicates().shape[0])
3/29:
# determine the number of distinct
print('--- Distinct Categories')
print('Level 1: %d' % df_source[['Level_1']].drop_duplicates().shape[0])
print('Level 2: %d' % df_source[['Level_2']].drop_duplicates().shape[0])
print('Level 3: %d' % df_source[['Level_3']].drop_duplicates().shape[0])

print('--- Unique Combinatios')
print('Level 2 & 3: %d' % df_source[['Level_2', 'Level_3']].drop_duplicates().shape[0])
3/30:
# determine the number of distinct
print('--- Distinct Categories')
print('Level 1: %d' % df_source[['Level_1']].drop_duplicates().shape[0])
print('Level 2: %d' % df_source[['Level_2']].drop_duplicates().shape[0])
print('Level 3: %d' % df_source[['Level_3']].drop_duplicates().shape[0])
print()

print('--- Unique Combinatios')
print('Level 2 & 3: %d' % df_source[['Level_2', 'Level_3']].drop_duplicates().shape[0])
3/31:
# determine the number of distinct
print('--- Distinct Categories')
print('Level 1: %d' % df_source[['Level_1']].drop_duplicates().shape[0])
print('Level 2: %d' % df_source[['Level_2']].drop_duplicates().shape[0])
print('Level 3: %d' % df_source[['Level_3']].drop_duplicates().shape[0])
print()

print('--- Unique Combinatios')
print('Level 2 & 3 : %d' % df_source[['Level_2', 'Level_3']].drop_duplicates().shape[0])
print('Total       : %d' % df_source[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])
3/32:
import pandas as pd
import plotly.express as px
3/33:
# load the dataset
#source_path = 'data/raw/product-cat-dataset.csv'
source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
3/34: df_source.head()
3/35:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
3/36:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
3/37:
# determine the number of distinct
print('--- Distinct Categories')
print('Level 1: %d' % df_source[['Level_1']].drop_duplicates().shape[0])
print('Level 2: %d' % df_source[['Level_2']].drop_duplicates().shape[0])
print('Level 3: %d' % df_source[['Level_3']].drop_duplicates().shape[0])
print()

print('--- Unique Combinatios')
print('Level 2 & 3 : %d' % df_source[['Level_2', 'Level_3']].drop_duplicates().shape[0])
print('Total       : %d' % df_source[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])
3/38: # Check if data has missing values in the Description column
3/39: # Deal with missing values
3/40: # Apply to Level_1
3/41: # Apply to Level_2
3/42: # Apply to Level_3
3/43:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
3/44:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
3/45:
# determine the number of distinct
print('--- Distinct Categories')
print('Level 1: %d' % df_source[['Level_1']].drop_duplicates().shape[0])
print('Level 2: %d' % df_source[['Level_2']].drop_duplicates().shape[0])
print('Level 3: %d' % df_source[['Level_3']].drop_duplicates().shape[0])
print()

print('--- Unique Combinatios')
print('Level 2 & 3 : %d' % df_source[['Level_2', 'Level_3']].drop_duplicates().shape[0])
print('Total       : %d' % df_source[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])
3/46:
# determine the number of distinct
print('--- Distinct Categories')
print('Level 1: %d' % df_source[['Level_1']].drop_duplicates().shape[0])
print('Level 2: %d' % df_source[['Level_2']].drop_duplicates().shape[0])
print('Level 3: %d' % df_source[['Level_3']].drop_duplicates().shape[0])
print()

print('--- Unique Combinatios')
print('Level 2 & 3 : %d' % df_source[['Level_2', 'Level_3']].drop_duplicates().shape[0])
print('Total       : %d' % df_source[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])
3/47: f_source[['Level_1', 'Level_2', 'Level_3']]
3/48: df_source[['Level_1', 'Level_2', 'Level_3']]
3/49: df_source[['Level_1', 'Level_2', 'Level_3']].drop_duplicates()
3/50:
# determine the number of distinct
print('--- Distinct Categories')
print('Level 1        : %d' % df_source[['Level_1']].drop_duplicates().shape[0])
print('Level 2        : %d' % df_source[['Level_2']].drop_duplicates().shape[0])
print('Level 3        : %d' % df_source[['Level_3']].drop_duplicates().shape[0])
print('Level 1, 2 & 3 : %d' % df_source[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])
3/51:
# determine the number of distinct
print('--- Distinct Categories')
print('Level 1 : %d' % df_source[['Level_1']].drop_duplicates().shape[0])
print('Level 2 : %d' % df_source[['Level_2']].drop_duplicates().shape[0])
print('Level 3 : %d' % df_source[['Level_3']].drop_duplicates().shape[0])
print('All     : %d' % df_source[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])
3/52:
import pandas as pd
import plotly.express as px
3/53:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
3/54: df_source.head()
3/55:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
3/56:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
3/57:
# determine the number of distinct
print('--- Distinct Categories ---')
print('Level 1 : %d' % df_source[['Level_1']].drop_duplicates().shape[0])
print('Level 2 : %d' % df_source[['Level_2']].drop_duplicates().shape[0])
print('Level 3 : %d' % df_source[['Level_3']].drop_duplicates().shape[0])
print('All     : %d' % df_source[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])
3/58: # Check if data has missing values in the Description column
3/59: # Deal with missing values
3/60: # Apply to Level_1
3/61: # Apply to Level_2
3/62: # Apply to Level_3
3/63:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
3/64:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
3/65:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])
3/66:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
3/67: df_source.describe()
3/68: df_source.isna()
3/69: df_source.isnull()
3/70: df_source.isnull().sum()
3/71: df_source.isna().sum()
3/72: df_source.isna().sum()
3/73: df_source.isna().sum()
3/74: df_source[df_source.isna()]
3/75: df_source.isna().sum()
3/76: df_source[df_source.isna()]
3/77: df_source[df_source.isna().index]
3/78: df_source.index[df.isnull().any(axis=1)]
3/79: df_source.index[df_source.isnull().any(axis=1)]
3/80: df_source[df_source.index[df_source.isnull().any(axis=1)]]
3/81: df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
3/82:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
3/83:
import pandas as pd
import plotly.express as px
3/84:
# load the dataset
#source_path = 'data/raw/product-cat-dataset.csv'
source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
3/85: df_source.head()
3/86:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
3/87:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
3/88:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
3/89: df_source.isna().sum()
3/90:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
3/91: # Check if data has missing values in the Description column
3/92: # Deal with missing values
3/93: # Apply to Level_1
3/94: # Apply to Level_2
3/95: # Apply to Level_3
3/96:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
3/97:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
3/98:
import pandas as pd
import plotly.express as px
3/99:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
3/100: df_source.head()
3/101:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
3/102:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
3/103:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
3/104: df_source.isna().sum()
3/105:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
3/106: # Check if data has missing values in the Description column
3/107: # Deal with missing values
3/108: # Apply to Level_1
3/109: # Apply to Level_2
3/110: # Apply to Level_3
3/111:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
3/112:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
3/113:
# Check if data has missing values in the Description column
df_source.isna().sum()
3/114:
# Check if data has missing values in the Description column
df_source.isna().sum()
3/115:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
3/116:
# Deal with missing values
df_clean = df_source.dropna()
3/117:
# Deal with missing values
df_clean = df_source.dropna()
df_clean = df_source.dropna()
df_clean.shape
3/118:
# Deal with missing values
df_clean = df_source.dropna()
df_clean = df_source.dropna()
df_clean.shape
3/119:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
3/120:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
3/121:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
3/122:
# convert the levels to categories
df_clean.Level_1.astype('category')
3/123:
# convert the levels to categories
df_clean.Level_1 = df_clean.Level_1.astype('category')
3/124:
# convert the levels to categories
df_clean.Level_1 = df_clean.Level_1.astype('category')
df_clean.Level_2 = df_clean.Level_1.astype('category')
df_clean.Level_3 = df_clean.Level_1.astype('category')

df_clean.info()
 4/1:
import pandas as pd
import plotly.express as px
 4/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
 4/3: df_source.head()
 4/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
 4/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
 4/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
 4/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
 4/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
 4/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
4/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
4/11:
# convert the levels to categories
df_clean.Level_1 = df_clean.Level_1.astype('category')
df_clean.Level_2 = df_clean.Level_1.astype('category')
df_clean.Level_3 = df_clean.Level_1.astype('category')

df_clean.info()
4/12: # Apply to Level_1
4/13: # Apply to Level_2
4/14: # Apply to Level_3
4/15:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
4/16:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
4/17:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
level_cols[level_cols] = level_cols[level_cols].astype('category')


df_clean.info()
4/18:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')


df_clean.info()
 5/1:
import pandas as pd
import plotly.express as px
 5/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
 5/3: df_source.head()
 5/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
 5/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
 5/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
 5/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
 5/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
 5/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
5/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
5/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')


df_clean.info()
5/12: # Apply to Level_1
5/13: # Apply to Level_2
5/14: # Apply to Level_3
5/15:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
5/16:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
 6/1:
import pandas as pd
import plotly.express as px
 6/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
 6/3: df_source.head()
 6/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
 6/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
 6/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
 6/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
 6/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
 6/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
6/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
6/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
6/12: # Apply to Level_1
6/13: # Apply to Level_2
6/14: # Apply to Level_3
6/15:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
6/16:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
6/17:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
print(df_clean.info())
6/18:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
6/19:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
6/20:
import pandas as pd
import plotly.express as px
6/21:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
6/22: df_source.head()
6/23:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
6/24:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
6/25:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
6/26:
# Check if data has missing values in the Description column
df_source.isna().sum()
6/27:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
6/28:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
6/29:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
6/30:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
6/31: # Apply to Level_1
6/32: # Apply to Level_2
6/33: # Apply to Level_3
6/34:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
6/35:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
6/36:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
6/37:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
6/38:
import pandas as pd
import plotly.express as px
6/39:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
6/40: df_source.head()
6/41:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
6/42:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
6/43:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
6/44:
# Check if data has missing values in the Description column
df_source.isna().sum()
6/45:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
6/46:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
6/47:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
6/48:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
6/49: # Apply to Level_1
6/50: # Apply to Level_2
6/51: # Apply to Level_3
6/52:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
6/53:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
6/54: df_source.Level_1.value_counts
6/55: df_source.Level_1.value_counts()
6/56: df_source.Level_1.value_counts().tail()
6/57: df_source.Level_1.value_counts(ascending=True)
6/58: df_source.Level_2.value_counts(ascending=True)
6/59: df_source.Level_2.value_counts(ascending=True).to_frame()
6/60: df_source.Level_2.value_counts(.to_frame()
6/61: df_source.Level_2.value_counts().to_frame()
6/62: df_source.Level_2.value_counts().to_frame()
6/63:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    data[column].value_counts().to_frame()
6/64: remove_small_categories(df_clean, 'Level_1', 10)
6/65:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    data[column].value_counts().to_frame().head()

remove_small_categories(df_clean, 'Level_1', 10)
6/66:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    data[column].value_counts().to_frame().head()

remove_small_categories(df_clean, 'Level_1', 10)
6/67:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    print(data[column].value_counts().to_frame())

remove_small_categories(df_clean, 'Level_1', 10)
6/68:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    data[column].value_counts().to_frame()

remove_small_categories(df_clean, 'Level_1', 10).head()
6/69:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    return data[column].value_counts().to_frame()

remove_small_categories(df_clean, 'Level_1', 10)
6/70:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame()

remove_small_categories(df_clean, 'Level_1', 10)
6/71:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame()
    return df_counts

remove_small_categories(df_clean, 'Level_1', 10)
6/72:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame(columns=['count'])
    return df_counts[]

remove_small_categories(df_clean, 'Level_1', 10)
6/73:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame(columns=['count'])
    return df_counts

remove_small_categories(df_clean, 'Level_1', 10)
6/74:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame(column=['count'])
    return df_counts

remove_small_categories(df_clean, 'Level_1', 10)
6/75:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame()
    return df_counts

remove_small_categories(df_clean, 'Level_1', 10)
6/76:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    return df_counts

remove_small_categories(df_clean, 'Level_1', 10)
6/77:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    return df_counts[df_counts.counts < n]

remove_small_categories(df_clean, 'Level_1', 10)
6/78:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    return df_counts[df_counts.counts < n]

remove_small_categories(df_clean, 'Level_1', 500)
6/79:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    return df_counts[df_counts.counts < n]

remove_small_categories(df_clean, 'Level_1', 600)
6/80:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    return df_counts[df_counts.counts < n]

remove_small_categories(df_clean, 'Level_2', 600)
6/81:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    return df_counts[df_counts.counts < n]

remove_small_categories(df_clean, 'Level_2', 10)
6/82:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = df_counts[df_counts.counts < n]

remove_small_categories(df_clean, 'Level_2', 10)
6/83:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = df_counts[df_counts.counts < n]
    return remove_list

remove_small_categories(df_clean, 'Level_2', 10)
6/84:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = df_counts[df_counts.counts < n].index
    return remove_list

remove_small_categories(df_clean, 'Level_2', 10)
6/85:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = df_counts[df_counts.counts < n].index.values
    return remove_list

remove_small_categories(df_clean, 'Level_2', 10)
6/86:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = list(df_counts[df_counts.counts < n].index.values)
    return remove_list

remove_small_categories(df_clean, 'Level_2', 10)
6/87:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    return remove_list

remove_small_categories(df_clean, 'Level_2', 10)
6/88:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('@column is in @remove_list')

remove_small_categories(df_clean, 'Level_2', 10)
6/89:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('@column contains @remove_list')

remove_small_categories(df_clean, 'Level_2', 10)
6/90:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('@column == "" ')

remove_small_categories(df_clean, 'Level_2', 10)
6/91:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s contains @remove_list' % column)

remove_small_categories(df_clean, 'Level_2', 10)
6/92:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s in @remove_list' % column)

remove_small_categories(df_clean, 'Level_2', 10)
6/93:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('~%s in @remove_list' % column)

remove_small_categories(df_clean, 'Level_2', 10)
6/94:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)

remove_small_categories(df_clean, 'Level_2', 10)
6/95:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s in @remove_list' % column)

remove_small_categories(df_clean, 'Level_2', 10)
6/96:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s 10 in @remove_list' % column)

remove_small_categories(df_clean, 'Level_2', 10)
6/97:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)

remove_small_categories(df_clean, 'Level_2', 10)
6/98:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s in @remove_list' % column)

remove_small_categories(df_clean, 'Level_2', 10)
6/99: df_clean.Level_2.value_counts()
6/100: df_clean.Level_2.value_counts(ascending=True)
6/101: df_clean.Level_2.value_counts(ascending=True).to_frame
6/102: df_clean.Level_2.value_counts(ascending=True).to_frame()
6/103: df_clean.Level_2.value_counts(ascending=True).to_frame().head()
6/104:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s in @remove_list' % column)

remove_small_categories(df_clean, 'Level_2', 10)
6/105:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)

remove_small_categories(df_clean, 'Level_2', 10)
6/106:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
6/107:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
 7/1:
import pandas as pd
import plotly.express as px
 7/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
 7/3: df_source.head()
 7/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
 7/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
 7/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
 7/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
 7/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
 7/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
7/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
7/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
7/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
7/13:
# Apply to Level_1 
remove_small_categories(df_clean, 'Level_1', 10)
7/14: # Apply to Level_2
7/15: # Apply to Level_3
7/16:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
7/17:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
7/18:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
7/19:
# Apply to Level_1 
remove_small_categories(df_clean, 'Level_1', 10)
7/20:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
7/21:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
7/22:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
7/23:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
 8/1:
import pandas as pd
import plotly.express as px
 8/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
 8/3: df_source.head()
 8/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
 8/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
 8/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
 8/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
 8/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
 8/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
8/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
8/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
8/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
8/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
8/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
8/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
8/16:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # write steps here
    
    return tokenised
8/17:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case
    lower = lower(ext)
    # write steps here
    
    return tokenised
8/19:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case
    lower = lower(ext)
    # write steps here
    
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/20:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case
    lower = text.lower()
    # write steps here
    
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/21:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case
    lower = text.lower()
    # write steps here
    
    tokenised = None
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/22:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
 from nltk.tokenize import word_tokenize
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & split the text into tokens
    lower = text.lower()
    tokens = 

    
    tokenised = None
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/23:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & split the text into tokens
    lower = text.lower()
    tokens = 

    
    tokenised = None
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/24:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & split the text into tokens
    lower = text.lower()
    tokens = word_tokenize()lower

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/25:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & split the text into tokens
    lower = text.lower()
    tokens = word_tokenize()

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/26:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & split the text into tokens
    lower = text.lower()
    tokens = word_tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/27:
import nltk
nltk.download('punkt')

from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & split the text into tokens
    lower = text.lower()
    tokens = word_tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/28:
import nltk
nltk.download('punkt')

from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & split the text into tokens
    lower = text.lower()
    tokens = word_tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/29:
import nltk
nltk.download('punkt')

from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & split the text into tokens
    lower = text.lower()
    tokens = word_tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/30:
import nltk
nltk.download('punkt')

from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import RegexpTokenizer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case
    lower = text.lower()

    # split the text into tokens
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/31:
import nltk
nltk.download('punkt')

from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import RegexpTokenizer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case
    lower = text.lower()

    # split the text into tokens
    tokens = word_tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/32:
import nltk
nltk.download('punkt')

from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & split the text into tokens
    lower = text.lower()
    tokens = word_tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/33:
import nltk
nltk.download('punkt')

from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import RegexpTokenizer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case
    lower = text.lower()

    # split the text into tokens
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/34:
import nltk
nltk.download('punkt')

from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import RegexpTokenizer
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case
    lower = text.lower()

    # split the text into tokens
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(lower)
        
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/35:
import nltk
nltk.download('punkt')

from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & split the text into tokens
    lower = text.lower()
    tokens = word_tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/36:
import nltk
nltk.download('punkt')
8/37:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string
imot

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/38:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string
import regexp as re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/39:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(lower)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/40:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/41:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/42:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    text re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/43:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/44:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/45:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/46:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/47:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    
    tokenised = tokens
    return tokenised

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/48:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    return ngrams(tokens, n)

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/49:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    return list(ngrams(tokens, n))

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/50:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/51:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/52:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    tokens = [ps.stem(token) for token in tokens]

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/53:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token) for token in tokens]

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/54:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token) + '!'  for token in tokens]

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/55:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]

process_text("Here we're testing the process_text function, results are as follows:", n = 3)
8/56:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
8/57:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
 9/1:
import pandas as pd
import plotly.express as px
 9/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
 9/3: df_source.head()
 9/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
 9/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
 9/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
 9/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
 9/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
 9/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
9/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
9/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
9/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
9/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
9/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
9/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
9/16:
import nltk
nltk.download('punkt')
9/17:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
9/18:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/19:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
9/20:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
9/21:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
9/22:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
9/23:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
9/24:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/25:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
9/26: count_vectorizer = CountVectorizer()
9/27: from sklearn.feature_extraction.text import CountVectorizer
9/28: count_vectorizer = CountVectorizer()
9/29:
# test text
test_grams = [
    process_text("Here we're testing the process_text function, results are as follows:", n=3),
    process_text("Here we're testing the process_text function, results are as follows:", n=3),
]

count_vectorizer = CountVectorizer()
count_vectorizer.fit(test_grams)
9/30:
# test text
test_grams = [
    process_text("Here we're testing the process_text function, results are as follows:", n=3),
    process_text("Here we're testing the process_text function, results are as follows:", n=3),
]

count_vectorizer = CountVectorizer()
count_vectorizer.fit(test_grams, lower=False)
9/31:
# test text
test_grams = [
    process_text("Here we're testing the process_text function, results are as follows:", n=3),
    process_text("Here we're testing the process_text function, results are as follows:", n=3),
]

count_vectorizer = CountVectorizer()
count_vectorizer.fit(test_grams, lowercase=False)
9/32:
# test text
test_grams = [
    process_text("Here we're testing the process_text function, results are as follows:", n=3),
    process_text("Here we're testing the process_text function, results are as follows:", n=3),
]

count_vectorizer = CountVectorizer(lowercase=False)
count_vectorizer.fit(test_grams)
9/33:
# test text
test_grams = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(preprocessor=process_text)
count_vectorizer.fit(test_grams)
9/34:
# test text
test_text = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(preprocessor=process_text)
count_vectorizer.fit(test_grams)
9/35:
# test text
test_text = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(preprocessor=process_text)
count_vectorizer.fit(test_text)
9/36:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    return tokens

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
9/37:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/38:
# test text
test_text = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(preprocessor=process_text)
count_vectorizer.fit(test_text)
9/39:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
9/40:
# test text
test_text = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(preprocessor=process_text)
count_vectorizer.fit(test_text)
9/41:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/42:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
9/43:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/44:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/45:
# test text
test_text = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(preprocessor=process_text)
count_vectorizer.fit(test_text)
9/46:
# test text
test_text = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(preprocessor=process_text, analyzer=lambda x:x)
count_vectorizer.fit(test_text)
9/47:
# test text
test_text = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer.fit(test_text)
9/48:
# test text
test_text = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer.fit(test_text)
count_vect.get_feature_names()
9/49:
# test text
test_text = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer.fit(test_text)
count_vectorizer.get_feature_names()
9/50:
# test text
test_text = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer.fit_transform(test_text)
count_vectorizer.get_feature_names()
9/51:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

#corpus = [build_ngrams(document) for document in test_docs]

#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer.fit_transform(test_docs)
count_vectorizer.get_feature_names()
9/52:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [build_ngrams(document) for document in test_docs]

#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer.fit_transform(test_docs)
count_vectorizer.get_feature_names()
9/53:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]

#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer.fit_transform(test_docs)
count_vectorizer.get_feature_names()
9/54:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]

#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/55:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]

#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer = CountVectorizer(analyzer=lambda x:x)
count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/56:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    #return [ ' '.join(grams) for grams in n_grams]
    return n_grams
9/57:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/58:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]

#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer = CountVectorizer(analyzer=lambda x:x)
count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/59:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    #return [ ' '.join(grams) for grams in n_grams]
    return list(n_grams)
9/60:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/61:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]

#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer = CountVectorizer(analyzer=lambda x:x)
count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/62:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    #return [ ' '.join(grams) for grams in n_grams]
    return list(n_grams)
9/63:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]

#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer = CountVectorizer(analyzer=lambda x:x)
count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/64:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]
corpus

#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer = CountVectorizer(analyzer=lambda x:x)
count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/65:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]
corpus

#count_vectorizer = CountVectorizer(analyzer=process_text)
#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
#count_vectorizer.get_feature_names()
9/66:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]
corpus[0]

#count_vectorizer = CountVectorizer(analyzer=process_text)
#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
#count_vectorizer.get_feature_names()
9/67:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    #return [ ' '.join(grams) for grams in n_grams]
    return list(n_grams)
9/68:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/69:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]
corpus[0]

#count_vectorizer = CountVectorizer(analyzer=process_text)
#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
#count_vectorizer.get_feature_names()
9/70:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]
process_text(test_docs[0])
#corpus[0]

#count_vectorizer = CountVectorizer(analyzer=process_text)
#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
#count_vectorizer.get_feature_names()
9/71:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document) for document in test_docs]
process_text(test_docs[0], n = 3)
#corpus[0]

#count_vectorizer = CountVectorizer(analyzer=process_text)
#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
#count_vectorizer.get_feature_names()
9/72:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document, 3) for document in test_docs]


#count_vectorizer = CountVectorizer(analyzer=process_text)
#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
#count_vectorizer.get_feature_names()
9/73:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document, 3) for document in test_docs]


#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer = CountVectorizer(analyzer=lambda x:x)
count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/74:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
9/75:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/76:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/77:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document, 3) for document in test_docs]


#count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer = CountVectorizer(analyzer=lambda x:x)
count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/78:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document, 3) for document in test_docs]


count_vectorizer = CountVectorizer(analyzer=process_text)
#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/79:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

corpus = [process_text(document, 3) for document in test_docs]


count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer.fit_transform(test_docs)

#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/80:
import pandas as pd
import plotly.express as px
9/81:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
9/82: df_source.head()
9/83:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
9/84:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
9/85:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
9/86:
# Check if data has missing values in the Description column
df_source.isna().sum()
9/87:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
9/88:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
9/89:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
9/90:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
9/91:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
9/92:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
9/93:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
9/94:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
9/95:
import nltk
nltk.download('punkt')
9/96:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
9/97:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/98:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
9/99:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
9/100: from sklearn.feature_extraction.text import CountVectorizer
9/101:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

#corpus = [process_text(document, 3) for document in test_docs]


count_vectorizer = CountVectorizer(analyzer=process_text)
count_vectorizer.fit_transform(test_docs)

#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/102:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
9/103:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
9/104:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

#corpus = [process_text(document, 3) for document in test_docs]


count_vectorizer = CountVectorizer(analyzer=lamda x:process_text(x, 3))
count_vectorizer.fit_transform(test_docs)

#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/105:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

#corpus = [process_text(document, 3) for document in test_docs]


count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
count_vectorizer.fit_transform(test_docs)

#count_vectorizer = CountVectorizer(analyzer=lambda x:x)
#count_vectorizer.fit_transform(corpus)
count_vectorizer.get_feature_names()
9/106:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/107:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
count_vectorizer.fit_transform(test_docs)
count_vectorizer.get_feature_names()
9/108:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
bow = count_vectorizer.fit_transform(test_docs)
count_vectorizer.get_feature_names()
9/109:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
bow = count_vectorizer.fit_transform(test_docs)

print(count_vectorizer.get_feature_names())
9/110:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
bow = count_vectorizer.fit_transform(test_docs)

print(count_vectorizer.get_feature_names())
print(bow)
9/111:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
bow = count_vectorizer.fit_transform(test_docs)

print(count_vectorizer.get_feature_names())
print(bow.toarray())
9/112:
# test text
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
bow = count_vectorizer.fit_transform(test_docs)

print(count_vectorizer.get_feature_names())
print(bow.toarray())
9/113: bow = count_vectorizer.fit_transform(df_clean.Description)
9/114:
count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
bow = count_vectorizer.fit_transform(df_clean.Description)
9/115:
print(count_vectorizer.get_feature_names())
print(bow.toarray())
9/116:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
9/117:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)

print(vectorier.get_feature_names())
print(bow.toarray())
9/118:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)

print(vectorier.get_feature_names_out())
print(bow.toarray())
9/119:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)

print(vectorier.get_feature_names())
print(bow.toarray())
9/120:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)

print(vectorier.get_feature_names_out())
print(bow.toarray())
9/121:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)

print(vectorier.get_feature_names())

print(bow.toarray())
9/122:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)

print(vectorier.get_feature_names())

print(bow.toarray())
9/123:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)

print(vectorier.get_feature_names())
print(vectorier.get_feature_names_out())
print(bow.toarray())
9/124:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)

print(vectorier.get_feature_names())
print(vectorier.get_feature_names_out())
print(bow.toarray())
9/125:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
9/126:
count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
bow = count_vectorizer.fit_transform(df_clean.Description)
9/127:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
9/128:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
9/129:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
9/130: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
9/131:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer=TfidfTransformer()
9/132:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
tfidf_transformer.fit_transform(bow)
9/133:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
9/134:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
9/135:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
9/136:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
9/137:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
9/138:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
9/139: text_tfidf.shape
9/140:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
9/141:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
9/142:
# Here is an example function call
process_text("Here we're testing the process_text 12a function, results are as follows:", n = 3)
9/143:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^-9A-Za-z ]", "" , text)
    #text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
9/144:
# Here is an example function call
process_text("Here we're testing the process_text 12a function, results are as follows:", n = 3)
9/145:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^0-9A-Za-z ]", "" , text)
    #text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
9/146:
# Here is an example function call
process_text("Here we're testing the process_text 12a function, results are as follows:", n = 3)
9/147:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
9/148:
# Here is an example function call
process_text("Here we're testing the process_text 12a function, results are as follows:", n = 3)
9/149:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
9/150:
# Here is an example function call
process_text("Here we're testing the process_text 12a function, results are as follows:", n = 3)
9/151:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
10/1:
import pandas as pd
import plotly.express as px
10/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
10/3: df_source.head()
10/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
10/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
10/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
10/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
10/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
10/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
10/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
10/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
10/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
10/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
10/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
10/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
10/16:
import nltk
nltk.download('punkt')
10/17:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
10/18:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
10/19:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
10/20:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
10/21: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
10/22:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
10/23:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
10/24:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
10/25:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
10/26:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
10/27: text_tfidf.shape
10/28:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
10/29: # Train/Test split
10/30:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
10/31:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
10/32:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
10/33:
# reset the index
df_clean.head()
10/34:
# reset the index
df_clean.reset_index(inplace=True)
df_clean.head()
11/1:
import pandas as pd
import plotly.express as px
11/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
11/3: df_source.head()
11/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
11/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
11/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
11/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
11/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
11/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
11/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
11/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
11/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
11/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
11/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
11/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
11/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
11/17:
import nltk
nltk.download('punkt')
11/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
11/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
11/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
11/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
11/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
11/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
11/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
11/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
11/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
11/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
11/28: text_tfidf.shape
11/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
11/30:
# combine the training features with the original cleanded data frame
df_featurized = df_clean[[]]
11/31: # Train/Test split
11/32:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
11/33:
# combine the training features with the original cleanded data frame
df_featurized = df_clean.join(text_tfidf)
df_featurized.head()
12/1:
import pandas as pd
import plotly.express as px
12/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
12/3: df_source.head()
12/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
12/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
12/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
12/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
12/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
12/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
12/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
12/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
12/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
12/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
12/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
12/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
12/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
12/17:
import nltk
nltk.download('punkt')
12/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
12/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
12/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
12/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
12/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
12/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
12/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
12/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
12/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
12/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
12/28: text_tfidf.shape
12/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
12/30:
# combine the training features with the original cleanded data frame
#df_featurized = df_clean.join(text_tfidf)
#df_featurized.head()
12/31: df_featurized.shape
12/32:
# combine the training features with the original cleanded data frame
df_featurized = df_clean.join(text_tfidf)
df_featurized.head()
13/1:
import pandas as pd
import plotly.express as px
13/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
13/3: df_source.head()
13/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
13/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
13/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
13/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
13/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
13/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
13/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
13/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
13/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
13/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
13/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
13/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
13/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
13/17:
import nltk
nltk.download('punkt')
13/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
13/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
13/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
13/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
13/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
13/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
13/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
13/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
13/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
13/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
13/28: text_tfidf.shape
13/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
13/30:
# combine the training features with the original cleanded data frame
df_featurized = text_tfidf.join(df_clean)
df_featurized.head()
14/1:
import pandas as pd
import plotly.express as px
14/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
14/3: df_source.head()
14/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
14/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
14/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
14/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
14/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
14/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
14/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
14/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
14/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
14/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
14/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
14/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
14/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
14/17:
import nltk
nltk.download('punkt')
14/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
14/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
14/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
14/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
14/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
14/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
14/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
14/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
14/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
14/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
14/28: text_tfidf.shape
14/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
14/30:
# combine the training features with the original cleanded data frame
df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized.head()
15/1:
import pandas as pd
import plotly.express as px
15/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
15/3: df_source.head()
15/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
15/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
15/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
15/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
15/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
15/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
15/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
15/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
15/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
15/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
15/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
15/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
15/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
15/17:
import nltk
nltk.download('punkt')
15/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
15/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
15/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
15/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
15/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
15/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
15/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
15/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
15/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
15/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
15/28: text_tfidf.shape
15/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
15/30:
# combine the training features with the original cleanded data frame
#df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
#df_featurized.head()
15/31: text_tfidf.shape
15/32: # Train/Test split
15/33:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
15/34:
# Train/Test split
text_tfidf.sample(frac=0.5).shape
16/1:
import pandas as pd
import plotly.express as px
16/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
16/3: df_source.head()
16/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
16/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
16/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
16/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
16/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
16/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
16/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
16/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
16/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
16/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
16/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
16/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
16/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
16/17:
import nltk
nltk.download('punkt')
16/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
16/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
16/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
16/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
16/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
16/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
16/24:
import pandas as pd
import plotly.express as px
16/25:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
16/26: df_source.head()
16/27:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
16/28:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
16/29:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
16/30:
# Check if data has missing values in the Description column
df_source.isna().sum()
16/31:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
16/32:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
16/33:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
16/34:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
16/35:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
16/36:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
16/37:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
16/38:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
16/39:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
16/40:
import nltk
nltk.download('punkt')
16/41:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
16/42:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
16/43:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
16/44:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
16/45: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
16/46:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
16/47:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzier, bow = bow_transform(test_docs)
print(vectorier.get_feature_names_out())
print(bow.toarray())
16/48:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzier, bow = bow_transform(test_docs)
print(vectorzier.get_feature_names())
print(bow.toarray())
16/49:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
17/1:
import pandas as pd
import plotly.express as px
17/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
17/3: df_source.head()
17/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
17/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
17/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
17/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
17/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
17/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
17/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
17/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
17/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
17/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
17/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
17/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
17/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
17/17:
import nltk
nltk.download('punkt')
17/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
17/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
17/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
17/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
17/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
17/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
17/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
17/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
17/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
17/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
17/28: text_tfidf.shape
17/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
17/30:
# combine the training features with the original cleanded data frame
#df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
#df_featurized.head()
17/31: text_tfidf.shape
17/32:
# Train/Test split
text_tfidf.sample(frac=0.5).shape
17/33:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
18/1:
import pandas as pd
import plotly.express as px
18/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
18/3: df_source.head()
18/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
18/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
18/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
18/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
18/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
18/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
18/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
18/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
18/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
18/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
18/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
18/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
18/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
18/17:
import nltk
nltk.download('punkt')
18/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
18/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
18/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
18/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
18/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
18/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
18/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
18/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
18/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
18/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
18/28: text_tfidf.shape
18/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
18/30:
# combine the training features with the original cleanded data frame
df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized.head()
19/1:
df_featurized = pd.merge(df_clean, text_tfidf)
df_featurized.head()
20/1:
import pandas as pd
import plotly.express as px
20/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
20/3: df_source.head()
20/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
20/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
20/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
20/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
20/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
20/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
20/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
20/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
20/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
20/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
20/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
20/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
20/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
20/17:
import nltk
nltk.download('punkt')
20/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
20/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
20/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
20/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
20/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
20/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
20/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
21/1:
import pandas as pd
import plotly.express as px
21/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
21/3: df_source.head()
21/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
21/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
21/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
21/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
21/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
21/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
21/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
21/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
21/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
21/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
21/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
21/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
21/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
21/17:
import nltk
nltk.download('punkt')
21/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
21/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
21/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
21/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
21/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
21/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
21/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
21/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
21/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
21/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
21/28: text_tfidf.shape
21/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
21/30:
df_featurized = pd.merge(df_clean, text_tfidf)
df_featurized.head()
21/31:
df_featurized = pd.merge(df_clean, text_tfidf, left_index=True)
df_featurized.head()
21/32:
df_featurized = pd.merge(df_clean, text_tfidf, left_index=True, right_index=True)
df_featurized.head()
23/1:
import pandas as pd
import plotly.express as px
23/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
23/3: df_source.head()
23/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
23/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
23/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
23/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
23/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
23/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
23/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
23/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
23/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
23/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
23/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
23/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
23/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
23/17:
import nltk
nltk.download('punkt')
23/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
23/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
23/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
23/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
23/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
23/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
23/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
23/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
23/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
23/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
23/28: text_tfidf.shape
23/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
23/30:
# combine the training features with the original cleanded data frame
df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized.head()
23/31: text_tfidf.shape
25/1:
import pandas as pd
import plotly.express as px
25/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
25/3: df_source.head()
25/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
25/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
25/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
25/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
25/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
25/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
25/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
25/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
25/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
25/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
25/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
25/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
25/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
25/17:
import nltk
nltk.download('punkt')
25/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
25/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
25/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
25/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
25/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
25/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
25/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
25/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
25/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
25/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
25/28: text_tfidf.shape
25/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
25/30:
# combine the training features with the original cleanded data frame
df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized.head()
25/31: df_featurized.shape
25/32:
# Train/Test split
#text_tfidf.sample(frac=0.5).shape
25/33:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
25/34: df_featurized.shape
26/1:
import pandas as pd
import plotly.express as px
26/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
26/3: df_source.head()
26/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
26/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
26/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
26/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
26/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
26/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
26/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
26/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
26/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
26/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
26/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
26/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
26/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
26/17:
import nltk
nltk.download('punkt')
26/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
26/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
26/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
26/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
26/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
26/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
26/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
26/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
26/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
26/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
26/28: text_tfidf.shape
26/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
26/30:
# combine the training features with the original cleanded data frame
df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized.head()
26/31: df_featurized.shape
26/32:
# Train/Test split
#text_tfidf.sample(frac=0.5).shape
26/33:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
26/34:
# remove the text_tfidf dataframe to save memory
text_tfidf = None
26/35:
# remove the text_tfidf dataframe to save memory
del text_tfidf
27/1:
import pandas as pd
import plotly.express as px
27/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
27/3: df_source.head()
27/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
27/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
27/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
27/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
27/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
27/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
27/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
27/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
27/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
27/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
27/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
27/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
27/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
27/17:
import nltk
nltk.download('punkt')
27/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
27/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
27/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
27/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
27/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
27/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
27/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
27/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
27/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
27/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
27/28: text_tfidf.shape
27/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
27/30:
# combine the training features with the original cleanded data frame
df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized.head()
27/31: df_featurized.shape
27/32:
# remove the text_tfidf dataframe to save memory
#del text_tfidf
27/33:
# Train/Test split
#text_tfidf.sample(frac=0.5).shape
27/34:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
27/35:
# remove the text_tfidf dataframe to save memory
del text_tfidf
27/36:
# remove the text_tfidf dataframe to save memory
del text_tfidf
del df_featurized
27/37:
# remove the text_tfidf dataframe to save memory
#del text_tfidf
del df_featurized
27/38:
# combine the training features with the original cleanded data frame
df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized.head()
28/1:
import pandas as pd
import plotly.express as px
28/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
28/3: df_source.head()
28/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
28/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
28/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
28/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
28/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
28/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
28/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
28/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
28/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
28/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
28/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
28/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
28/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
28/17:
import nltk
nltk.download('punkt')
28/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
28/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
28/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
28/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
28/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
28/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
28/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
28/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
28/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
28/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
28/28: text_tfidf.shape
28/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
28/30:
# combine the training features with the original cleanded data frame
df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized.head()
28/31: df_featurized.shape
28/32:
# remove the text_tfidf dataframe to save memory
#del text_tfidf
#del df_featurized
28/33:
# Train/Test split
#text_tfidf.sample(frac=0.5).shape
28/34:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
28/35: df_featurized.shape
28/36: df_featurized.shape
28/37: df_featurized.dtypes
29/1:
import pandas as pd
import plotly.express as px
29/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
29/3: df_source.head()
29/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
29/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
29/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
29/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
29/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
29/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
29/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
29/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
#df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
29/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
29/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
29/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
29/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
29/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
29/17:
import nltk
nltk.download('punkt')
29/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
29/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
29/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
29/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
29/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
29/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
29/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
29/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
29/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
29/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
29/28: text_tfidf.shape
29/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
29/30:
# combine the training features with the original cleanded data frame
df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized.head()
29/31: df_featurized.shape
29/32: df_featurized.dtypes
29/33:
# remove the text_tfidf dataframe to save memory
#del text_tfidf
#del df_featurized
29/34:
# Train/Test split
#text_tfidf.sample(frac=0.5).shape
29/35:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
29/36:
# combine the training features with the original cleanded data frame
#df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized.head()
29/37:
# combine the training features with the original cleanded data frame
#df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
df_featurized = df_clean[['Level_1']].join(text_tfidf)
df_featurized.head()
30/1:
import pandas as pd
import plotly.express as px
30/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
30/3: df_source.head()
30/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
30/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
30/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
30/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
30/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
30/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
30/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
30/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
30/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
30/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
30/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
30/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
30/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
30/17:
import nltk
nltk.download('punkt')
30/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
30/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
30/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
30/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
30/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
30/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
30/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
30/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
30/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
30/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
30/28: text_tfidf.shape
30/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
30/30:
# combine the training features with the original cleanded data frame
#df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
#df_featurized = df_clean[['Level_1']].join(text_tfidf)
#df_featurized.head()
30/31: # Train/Test split
30/32:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
30/33:
# Train/Test split
from sklearn.model_selection import train_test_split
30/34:
# Train/Test split
from sklearn.model_selection import train_test_split
y = df_clean

y.head()
30/35:
# Train/Test split
from sklearn.model_selection import train_test_split

y = df_clean

y.head()
30/36:
# Train/Test split
from sklearn.model_selection import train_test_split

y = df_clean.drop('Description')

y.head()
30/37:
# Train/Test split
from sklearn.model_selection import train_test_split

y = df_clean.drop(['Description'])

y.head()
30/38:
# Train/Test split
from sklearn.model_selection import train_test_split

y = df_clean.drop('Description', axis=0)

y.head()
30/39:
# Train/Test split
from sklearn.model_selection import train_test_split

y = df_clean.drop('Description', axis=1)

y.head()
30/40: df_clean.head*()
30/41: df_clean.head()
30/42:
# Train/Test split
from sklearn.model_selection import train_test_split

y = df_clean.drop('Description', axis=1)
X = text_tfidf
30/43:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train,X_test,y_train,y_test = train_test_split(X.index,y,test_size=0.3)
30/44:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train,X_test,y_train,y_test = train_test_split(X.index,y,test_size=0.3)

X_train
30/45:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)

X_test
30/46:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)

X_train
30/47:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)

X_train
y_train
30/48:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)

X_train
#y_train
30/49:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)

X_train
#y_train
30/50:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)

#X_train
y_train
30/51:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)

#X_train
y_train
30/52:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
#X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
31/1:
import pandas as pd
import plotly.express as px
31/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
31/3: df_source.head()
31/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
31/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
31/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
31/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
31/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
31/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
31/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
31/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
31/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
31/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
31/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
31/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
31/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
31/17:
import nltk
nltk.download('punkt')
31/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
31/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
31/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
31/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
31/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
31/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
31/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
31/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
31/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
31/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
31/28: text_tfidf.shape
31/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
31/30:
# combine the training features with the original cleanded data frame
#df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
#df_featurized = df_clean[['Level_1']].join(text_tfidf)
#df_featurized.head()
31/31:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
#X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
31/32:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
31/33:
# You might need to take classes as separate columns (depends on you how you do things)
class1 = y_train['Level_1'].astype(str)
class2 = y_train['Level_2'].astype(str)
class3 = y_train['Level_3'].astype(str)
31/34: # Create and save model for level 1
31/35: ## Create and save models for level 2
31/36: ## Create and save models for level 3
31/37:
# Creating an empty Dataframe with column names only (depends on you how you do things)
results = pd.DataFrame(columns=['Level1_Pred', 'Level2_Pred', 'Level3_Pred'])

## Here we reload the saved models and use them to predict the levels
# load model for level 1 (done for you)
with open('level1.pk', 'rb') as nb:
    model = pickle.load(nb)

## loop through the test data, predict level 1, then based on that predict level 2
## and based on level 2 predict level 3 (you need to load saved models accordingly)
31/38:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
31/39:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
#X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
31/40:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
31/41:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
31/42:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
31/43:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
#X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
33/1:
import pandas as pd
import plotly.express as px
33/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
33/3: df_source.head()
33/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
33/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
33/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
33/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
33/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
33/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
33/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
33/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
33/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
33/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
33/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
33/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
33/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
33/17:
import nltk
nltk.download('punkt')
33/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
33/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
33/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
33/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
33/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
33/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
33/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
33/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
33/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
33/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
33/28: text_tfidf.shape
33/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
33/30:
# combine the training features with the original cleanded data frame
#df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
#df_featurized = df_clean[['Level_1']].join(text_tfidf)
#df_featurized.head()
33/31:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
#X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
33/32:
del X
del text_tfidf
33/33:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
33/34:
# You might need to take classes as separate columns (depends on you how you do things)
class1 = y_train['Level_1'].astype(str)
class2 = y_train['Level_2'].astype(str)
class3 = y_train['Level_3'].astype(str)
33/35: # Create and save model for level 1
33/36: ## Create and save models for level 2
33/37: ## Create and save models for level 3
33/38:
# Creating an empty Dataframe with column names only (depends on you how you do things)
results = pd.DataFrame(columns=['Level1_Pred', 'Level2_Pred', 'Level3_Pred'])

## Here we reload the saved models and use them to predict the levels
# load model for level 1 (done for you)
with open('level1.pk', 'rb') as nb:
    model = pickle.load(nb)

## loop through the test data, predict level 1, then based on that predict level 2
## and based on level 2 predict level 3 (you need to load saved models accordingly)
33/39:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
34/1:
import pandas as pd
import plotly.express as px
34/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
34/3: df_source.head()
34/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
34/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
34/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
34/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
34/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
34/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
34/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
34/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
34/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
34/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
34/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
34/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
34/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
34/17:
import nltk
nltk.download('punkt')
34/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
34/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
34/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
34/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
34/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
34/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

vectorzer, bow = bow_transform(test_docs)
print(vectorzer.get_feature_names())
print(bow.toarray())
34/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
34/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
34/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
34/27:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
34/28: text_tfidf.shape
34/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
34/30:
# combine the training features with the original cleanded data frame
#df_featurized = df_clean[['Level_1', 'Level_2', 'Level_3']].join(text_tfidf)
#df_featurized = df_clean[['Level_1']].join(text_tfidf)
#df_featurized.head()
34/31:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
34/32: X.iloc[X_train]
34/33:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
34/34: X.iloc[X_train].shape()
34/35:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
34/36:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
34/37:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer, bow = bow_transform(df_clean.Description)
print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/1:
import pandas as pd
import plotly.express as px
36/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
36/3: df_source.head()
36/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
36/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
36/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
36/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
36/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
36/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
36/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
36/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
36/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
36/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
36/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
36/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
36/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
36/17:
import nltk
nltk.download('punkt')
36/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
36/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
36/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
36/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
36/22: from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
36/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, 3))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer, bow = bow_transform(test_docs)
print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/24:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/25:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
36/26:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
36/27: text_tfidf.shape
36/28:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
36/29:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
36/30:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description)
36/31:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/32:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
36/33:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
36/34: text_tfidf.shape
36/35:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
36/36:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
#text_tfidf = tfidf_transformer.fit_transform(bow)
text_tfidf = tfidf_transformer.transform(bow)
36/37:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
36/38:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description, 1)
36/39:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/40:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
36/41:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
36/42:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
36/43: text_tfidf.shape
36/44:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
36/45:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, n))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer, bow = bow_transform(test_docs)
print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/46:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, n))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer, bow = bow_transform(test_docs)
print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/47:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description, 1)
36/48:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/49:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
36/50:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
36/51: text_tfidf.shape
36/52:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
36/53:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description, 2)
36/54:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/55:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
36/56:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
36/57: text_tfidf.shape
36/58:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
36/59:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description, 1)
36/60:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/61:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
36/62:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
36/63: text_tfidf.shape
36/64:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
36/65:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description, 3)
36/66:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
36/67:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
36/68:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
text_tfidf = pd.DataFrame(text_tfidf.toarray())
36/69: text_tfidf.shape
36/70:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
36/71:
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.decomposition import TruncatedSVD
36/72:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=50, random_state=42)
36/73:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=50, random_state=42)
text_svd = svd.fit_transform(text_tfidf)
37/1:
import pandas as pd
import plotly.express as px
37/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
37/3: df_source.head()
37/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
37/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
37/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
37/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
37/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
37/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
37/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
37/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
37/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
37/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
37/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
37/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
37/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
37/17:
import nltk
nltk.download('punkt')
37/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
37/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
37/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
37/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
37/22:
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.decomposition import TruncatedSVD
37/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, n))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer, bow = bow_transform(test_docs)
print(count_vectorizer.get_feature_names())
print(bow.toarray())
37/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description, 3)
37/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
37/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
38/1:
import pandas as pd
import plotly.express as px
38/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
38/3: df_source.head()
38/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
38/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
38/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
38/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
38/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
38/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
38/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
38/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
38/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
38/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
38/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
38/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
38/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
38/17:
import nltk
nltk.download('punkt')
38/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
38/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
38/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
38/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
38/22:
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.decomposition import TruncatedSVD
38/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, n))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer, bow = bow_transform(test_docs)
print(count_vectorizer.get_feature_names())
print(bow.toarray())
38/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description, 3)
38/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
38/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
38/27:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=50, random_state=42)
text_svd = svd.fit_transform(text_tfidf)
38/28:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=50, random_state=42)
text_svd = svd.fit_transform(text_tfidf)

print(f"Total variance explained: {np.sum(svd.explained_variance_ratio_):.2f}")
38/29:
import pandas as pd
import numpy as np
import plotly.express as px
38/30:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=50, random_state=42)
text_svd = svd.fit_transform(text_tfidf)

print(f"Total variance explained: {np.sum(svd.explained_variance_ratio_):.2f}")
38/31:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=100, random_state=42)
text_svd = svd.fit_transform(text_tfidf)

print(f"Total variance explained: {np.sum(svd.explained_variance_ratio_):.2f}")
38/32:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=1000, random_state=42)
text_svd = svd.fit_transform(text_tfidf)

print(f"Total variance explained: {np.sum(svd.explained_variance_ratio_):.2f}")
38/33:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=2000, random_state=42)
text_svd = svd.fit_transform(text_tfidf)

print(f"Total variance explained: {np.sum(svd.explained_variance_ratio_):.2f}")
38/34: print("Total variance explained: %.2f" % svd.explained_variance_ratio_.sum())
38/35: print('Total variance explained: %.2f' % svd.explained_variance_ratio_.sum())
38/36:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
#text_tfidf = pd.DataFrame(text_tfidf.toarray())
text_tfidf = pd.DataFrame(text_svd.toarray())
38/37:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
#text_tfidf = pd.DataFrame(text_tfidf.toarray())
text_tfidf = pd.DataFrame(text_svd)
38/38: text_tfidf.shape
38/39:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
38/40:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=3000, random_state=42)
text_svd = svd.fit_transform(text_tfidf)

print('Total variance explained: %.2f' % svd.explained_variance_ratio_.sum())
38/41:
import pandas as pd
import numpy as np
import plotly.express as px
38/42:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
38/43: df_source.head()
38/44:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
38/45:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
38/46:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
38/47:
# Check if data has missing values in the Description column
df_source.isna().sum()
38/48:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
38/49:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
38/50:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
38/51:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
38/52:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
38/53:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
38/54:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
38/55:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
38/56:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
38/57:
import nltk
nltk.download('punkt')
38/58:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
38/59:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
38/60:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
38/61:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
38/62:
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.decomposition import TruncatedSVD
38/63:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, n))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer, bow = bow_transform(test_docs)
print(count_vectorizer.get_feature_names())
print(bow.toarray())
38/64:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description, 3)
38/65:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
38/66:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
38/67:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=3000, random_state=42)
text_svd = svd.fit_transform(text_tfidf)

print('Total variance explained: %.2f' % svd.explained_variance_ratio_.sum())
38/68:
# perform dimensionality reduction
svd = TruncatedSVD(n_components=2000, random_state=42)
text_svd = svd.fit_transform(text_tfidf)

print('Total variance explained: %.2f' % svd.explained_variance_ratio_.sum())
38/69:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
#text_tfidf = pd.DataFrame(text_tfidf.toarray())
text_tfidf = pd.DataFrame(text_svd)
38/70: text_tfidf.shape
38/71:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
38/72:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
#X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
38/73:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
#X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

X_train
#y_train
38/74:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
#X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
38/75: X.iloc[X_train].shape
38/76:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
38/77:
# You might need to take classes as separate columns (depends on you how you do things)
class1 = y_train['Level_1'].astype(str)
class2 = y_train['Level_2'].astype(str)
class3 = y_train['Level_3'].astype(str)
41/1:
import pandas as pd
import numpy as np
import plotly.express as px
41/2:
# load the dataset
source_path = 'data/raw/product-cat-dataset.csv'
#source_path = 'data/raw/product-category-dataset.csv'

df_source = pd.read_csv(source_path)
df_source.info()
41/3: df_source.head()
41/4:
# perform data grouping
source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])
source_grouped_count = source_grouped.size().to_frame(name = 'count')
source_grouped_count.head(10)
41/5:
# visualize the dataset hirachy
fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')
fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))
fig.show()
41/6:
def print_categories(data:pd.DataFrame):
    """
    Print a count of the distinct categories in the dataset.
    """
    # determine the number of distinct
    print('--- Distinct Categories ---')
    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])
    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])
    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])
    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])

# show the number of distinct categories in the dataset
print_categories(df_source)
41/7:
# Check if data has missing values in the Description column
df_source.isna().sum()
41/8:
# show the rows with missing values
df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]
41/9:
# Deal with missing values
df_clean = df_source.dropna()
df_clean.shape
41/10:
# show the number of caterories remaining after dropping null values
print_categories(df_clean)
41/11:
# convert the levels to categories
level_cols = ['Level_1', 'Level_2', 'Level_3']
df_clean[level_cols] = df_clean[level_cols].astype('category')
df_clean.info()
41/12:
def remove_small_categories(data:pd.DataFrame, column:str, n:int):
    """
    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.

    Returns:
        The original dataset with categories that have less than n rows have been removed.
    """
    df_counts = data[column].value_counts().to_frame('counts')
    remove_list = set(df_counts[df_counts.counts < n].index.values)
    
    return data.query('%s not in @remove_list' % column)
41/13:
# Apply to Level_1 
df_clean = remove_small_categories(df_clean, 'Level_1', 10)
df_clean.shape
41/14:
# Apply to Level_2
df_clean = remove_small_categories(df_clean, 'Level_2', 10)
df_clean.shape
41/15:
# Apply to Level_3
df_clean = remove_small_categories(df_clean, 'Level_3', 10)
df_clean.shape
41/16:
# reset the index
df_clean.reset_index(inplace=True, drop=True)
df_clean.head()
41/17:
import nltk
nltk.download('punkt')
41/18:
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
import string
import re

def process_text(text, n = 1):
    """
    Takes in a string of text, then performs the following:
    1. Convert text to lower case and remove all punctuation
    2. Optionally apply stemming
    3. Apply Ngram Tokenisation
    4. Returns the tokenised text as a list
    """
    # convert to lower case & 
    text = text.lower()

    # remove punctuation
    #text = re.sub("[^0-9A-Za-z ]", "" , text)
    text = re.sub("[^A-Za-z ]", "" , text)

    # split the text into tokens
    tokens = word_tokenize(text)

    # perform stemming
    ps = PorterStemmer()
    tokens = [ps.stem(token)  for token in tokens]

    #return ' '.join(tokens)

    # get the ngrams
    n_grams  = ngrams(tokens, n)

    # return the ngrams as a list of strings
    return [ ' '.join(grams) for grams in n_grams]
    #return list(n_grams)
41/19:
# Here is an example function call
process_text("Here we're testing the process_text function, results are as follows:", n = 3)
41/20:
# Results should look like this:
['here were test',
 'were test the',
 'test the processtext',
 'the processtext function',
 'processtext function result',
 'function result are',
 'result are as',
 'are as follow']
41/21:
# Might take a while...
# Here you apply the process_text function to the Description column of the data
# Then you pass the results to the bag of words tranformer
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
41/22:
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.decomposition import TruncatedSVD
41/23:
def bow_transform(corpus, n=3):
    """
    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.
    """
    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, n))
    bow = count_vectorizer.fit_transform(corpus)

    return count_vectorizer, bow

# test the functon
test_docs = [
    "Here we're testing the process_text function, results are as follows:",
    "Here you apply the process_text function to the Description column of the data"
]

count_vectorizer, bow = bow_transform(test_docs)
print(count_vectorizer.get_feature_names())
print(bow.toarray())
41/24:
# vectorize the description column in the dataset
count_vectorizer, bow = bow_transform(df_clean.Description, 3)
41/25:
#print(count_vectorizer.get_feature_names())
print(bow.toarray())
41/26:
# After that you pass the result of the previous step to sklearn's TfidfTransformer
# which will convert them into a feature matrix
# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
    
tfidf_transformer = TfidfTransformer()
text_tfidf = tfidf_transformer.fit_transform(bow)
41/27:
# perform dimensionality reduction
n_components = 50 #2000

svd = TruncatedSVD(n_components=n_components, random_state=42)
text_svd = svd.fit_transform(text_tfidf)

print('Total variance explained: %.2f' % svd.explained_variance_ratio_.sum())
41/28:
# The resulting matrix is in sparse format, we can transform it into dense
# Code prepared for you so you can see what results look like
#text_tfidf = pd.DataFrame(text_tfidf.toarray())
text_tfidf = pd.DataFrame(text_svd)
text_tfidf.shape
41/29:
# This is an example result, the matrix will contain lots of zero values, that is expected
# Some values will be non-zero
text_tfidf.head()
41/30:
from sklearn.model_selection import train_test_split

# get the dependent and indeendent variables
y = df_clean.drop('Description', axis=1)
X = text_tfidf

# Train/Test split
#X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

#X_train
y_train
41/31:
# You might need to reset index in each dataframe (depends on you how you do things)
# done for you to make it clearer
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)
41/32:
# You might need to take classes as separate columns (depends on you how you do things)
class1 = y_train['Level_1'].astype(str)
class2 = y_train['Level_2'].astype(str)
class3 = y_train['Level_3'].astype(str)
41/33: # Create and save model for level 1
41/34: ## Create and save models for level 2
41/35: ## Create and save models for level 3
41/36:
# Creating an empty Dataframe with column names only (depends on you how you do things)
results = pd.DataFrame(columns=['Level1_Pred', 'Level2_Pred', 'Level3_Pred'])

## Here we reload the saved models and use them to predict the levels
# load model for level 1 (done for you)
with open('level1.pk', 'rb') as nb:
    model = pickle.load(nb)

## loop through the test data, predict level 1, then based on that predict level 2
## and based on level 2 predict level 3 (you need to load saved models accordingly)
   1: %history -g
   2: %history -g -f notebook_file.ipynb
