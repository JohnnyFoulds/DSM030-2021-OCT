{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppresse future warnings - unfortunately pycaret/sklearn simply does not obey this\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure loggin and test that it is working\n",
    "import logging\n",
    "logging.basicConfig(filename='cw01.log', filemode='w', level=logging.CRITICAL)\n",
    "logging.warning('Watch out!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and explore the data (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "source_path = 'data/raw/product-cat-dataset.csv'\n",
    "#source_path = 'data/raw/product-category-dataset.csv'\n",
    "\n",
    "df_source = pd.read_csv(source_path)\n",
    "df_source.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Levels\n",
    "\n",
    "Get an overview of hierarchy of levels used in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform data grouping\n",
    "source_grouped = df_source.groupby(['Level_1', 'Level_2', 'Level_3'])\n",
    "source_grouped_count = source_grouped.size().to_frame(name = 'count')\n",
    "source_grouped_count.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataset hierarchy\n",
    "fig = px.treemap(source_grouped_count.reset_index(), path=['Level_1', 'Level_2', 'Level_3'], values='count')\n",
    "fig.update_layout(margin = dict(t=25, l=25, r=25, b=25))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_categories(data:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Print a count of the distinct categories in the dataset.\n",
    "    \"\"\"\n",
    "    # determine the number of distinct\n",
    "    print('--- Distinct Categories ---')\n",
    "    print('Level 1 : %d' % data[['Level_1']].drop_duplicates().shape[0])\n",
    "    print('Level 2 : %d' % data[['Level_2']].drop_duplicates().shape[0])\n",
    "    print('Level 3 : %d' % data[['Level_3']].drop_duplicates().shape[0])\n",
    "    print('All     : %d' % data[['Level_1', 'Level_2', 'Level_3']].drop_duplicates().shape[0])\n",
    "\n",
    "# show the number of distinct categories in the dataset\n",
    "print_categories(df_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with Missing Data (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data has missing values in the Description column\n",
    "df_source.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the rows with missing values\n",
    "df_source.iloc[df_source.index[df_source.isnull().any(axis=1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with missing values\n",
    "df_clean = df_source.dropna()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number of categories remaining after dropping null values\n",
    "print_categories(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Classes where the number of instances is < 10 (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small_categories(data:pd.DataFrame, column:str, n:int):\n",
    "    \"\"\"\n",
    "    This function find instances in the dataset provided and find instances in the specified column that has less than n rows.\n",
    "\n",
    "    Returns:\n",
    "        The original dataset with categories that have less than n rows have been removed.\n",
    "    \"\"\"\n",
    "    df_counts = data[column].value_counts().to_frame('counts')\n",
    "    remove_list = set(df_counts[df_counts.counts < n].index.values)\n",
    "    \n",
    "    return data.query('%s not in @remove_list' % column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to Level_1 \n",
    "df_clean = remove_small_categories(df_clean, 'Level_1', 10)\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to Level_2\n",
    "df_clean = remove_small_categories(df_clean, 'Level_2', 10)\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to Level_3\n",
    "df_clean = remove_small_categories(df_clean, 'Level_3', 10)\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the levels to categories\n",
    "level_cols = ['Level_1', 'Level_2', 'Level_3']\n",
    "df_clean[level_cols] = df_clean[level_cols].astype('category')\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index\n",
    "df_clean.reset_index(inplace=True, drop=True)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 1 distribution\n",
    "df_clean.Level_1.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 2 distribution\n",
    "df_clean.Level_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 3 distribution\n",
    "df_clean.Level_3.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's write a Function to Prepare Text (4 marks)\n",
    "We will apply it to our DataFrame later on\n",
    "\n",
    "* This function receives a text string and performs the following:\n",
    "* Convert text to lower case\n",
    "* Remove punctuation marks\n",
    "* Apply stemming using the popular Snowball or Porter Stemmer (optional)\n",
    "* Apply NGram Tokenisation\n",
    "* Return the tokenised text as a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import re\n",
    "\n",
    "def process_text(text, n = 1):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Convert text to lower case and remove all punctuation\n",
    "    2. Optionally apply stemming\n",
    "    3. Apply Ngram Tokenisation\n",
    "    4. Returns the tokenised text as a list\n",
    "    \"\"\"\n",
    "    # convert to lower case & \n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    #text = re.sub(\"[^0-9A-Za-z ]\", \"\" , text)\n",
    "    text = re.sub(\"[^A-Za-z ]\", \"\" , text)\n",
    "\n",
    "    # split the text into tokens\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # perform stemming\n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(token)  for token in tokens]\n",
    "\n",
    "    #return ' '.join(tokens)\n",
    "\n",
    "    # get the ngrams\n",
    "    n_grams  = ngrams(tokens, n)\n",
    "\n",
    "    # return the ngrams as a list of strings\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "    #return list(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example function call\n",
    "process_text(\"Here we're testing the process_text function, results are as follows:\", n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results should look like this:\n",
    "['here were test',\n",
    " 'were test the',\n",
    " 'test the processtext',\n",
    " 'the processtext function',\n",
    " 'processtext function result',\n",
    " 'function result are',\n",
    " 'result are as',\n",
    " 'are as follow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's apply TF-IDF to extract features from plain text (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might take a while...\n",
    "# Here you apply the process_text function to the Description column of the data\n",
    "# Then you pass the results to the bag of words tranformer\n",
    "# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_transform(corpus, n=3, max_features=None):\n",
    "    \"\"\"\n",
    "    Perform the bag-of-words transformation on the document corpus. The number of ngrams to use as tokens is specified using the n argument.\n",
    "    \"\"\"\n",
    "    count_vectorizer = CountVectorizer(analyzer=lambda x:process_text(x, n), max_features=max_features)\n",
    "    bow = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    return count_vectorizer, bow\n",
    "\n",
    "# test the function\n",
    "test_docs = [\n",
    "    \"Here we're testing the process_text function, results are as follows:\",\n",
    "    \"Here you apply the process_text function to the Description column of the data\"\n",
    "]\n",
    "\n",
    "count_vectorizer, bow = bow_transform(test_docs)\n",
    "print(count_vectorizer.get_feature_names())\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please Note\n",
    "During model evaluation it was found that using 3 ngrams is not ideal and better model performance is posible if ngrams are not created. When initially reading the coursework specification I was also puzzled by this approach as it is somehing I have never done in practise and could not wrap my head around how it would improve accuracy as a preprocessing step to TF-IDF. My intuition is that it is very likley to increase the number of tokens (and there by increase the curse of dimentionality) and make the matrix even more sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the description column in the dataset\n",
    "count_vectorizer, bow = bow_transform(df_clean.Description, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(count_vectorizer.get_feature_names())\n",
    "print('Feature Count:', len(bow.toarray()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use .transform on our Bag-of-Words (bow) transformed object and transform the entire DataFrame of text file contents. Let's go ahead and check out how the bag-of-words counts for the entire corpus in a large, sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that you pass the result of the previous step to sklearn's TfidfTransformer\n",
    "# which will convert them into a feature matrix\n",
    "# See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "    \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "text_tfidf = tfidf_transformer.fit_transform(bow)\n",
    "print('Feature Count:', len(text_tfidf.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform dimensionality reduction\n",
    "dim_reduction = True\n",
    "n_components = 50 #500 #100 #50 #2000\n",
    "\n",
    "if dim_reduction:\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    text_svd = svd.fit_transform(text_tfidf)\n",
    "\n",
    "    print('Total variance explained: %.2f' % svd.explained_variance_ratio_.sum())\n",
    "else:\n",
    "    text_svd = text_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting matrix is in sparse format, we can transform it into dense\n",
    "# Code prepared for you so you can see what results look like\n",
    "#text_tfidf = pd.DataFrame(text_tfidf.toarray())\n",
    "text_tfidf = pd.DataFrame(text_svd)\n",
    "text_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because dimensionality reduction was done on the TF-IDF features the matrix will not contain many zeros as per the original instructions. \n",
    "\n",
    "_I have tried to stick to the instructions as close I could, but many of the steps did not make 100% sense to me as I have done a number of NLP projects in the past and the approach is not what is typically seen in industry, so my thinking might have steered me away from the specific restrictions in this coursework. I can only hope that this does not negatively effect my grade._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example result, the matrix will contain lots of zero values, that is expected\n",
    "# Some values will be non-zero\n",
    "text_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the Data is Ready for Classifier Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train and Test sets (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get the dependent and indeendent variables\n",
    "y = df_clean.drop('Description', axis=1)\n",
    "X = text_tfidf\n",
    "\n",
    "# Train/Test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X.index,y,test_size=0.3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5429)\n",
    "\n",
    "print('Training Set : %d' % X_train.shape[0])\n",
    "print('Testing  Set : %d' % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to reset index in each dataframe (depends on you how you do things)\n",
    "# done for you to make it clearer\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to take classes as separate columns (depends on you how you do things)\n",
    "class1 = y_train['Level_1'].astype(str)\n",
    "class2 = y_train['Level_2'].astype(str)\n",
    "class3 = y_train['Level_3'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training for the three levels (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data:pd.DataFrame, target:str, session_id:int=None, exclude=None):\n",
    "    '''\n",
    "    Create a classification model for the dataset provided.\n",
    "\n",
    "    Parameters:\n",
    "        data: pandas.DataFrame\n",
    "            The DataFrame to build the model for.\n",
    "        target: str\n",
    "            The name of the target variable.\n",
    "        exclude: list of str, default = None\n",
    "            To omit certain models from training and evaluation, pass a list containing model id in the exclude parameter. \n",
    "    Returns:\n",
    "    '''\n",
    "    #from pycaret.classification import * \n",
    "\n",
    "    # create the classifier for the model\n",
    "    logging.info('Setting up the classifier...')\n",
    "    classifier = setup(\n",
    "        data = data, \n",
    "        target = target, \n",
    "        session_id=session_id,\n",
    "        fold = 10 if data.shape[0] * 0.7 > 10 else math.floor(data.shape[0] * 0.7) - 1,\n",
    "        verbose=False,\n",
    "        silent=True) \n",
    "\n",
    "    # if the dataset is too small knn can not be used\n",
    "    if data.shape[0] < 20 and exclude:\n",
    "        exclude = exclude + ['knn'] \n",
    "\n",
    "    # search for the best classifier\n",
    "    logging.info('Finding the best model...')\n",
    "    best = compare_models(\n",
    "        verbose=False,\n",
    "        exclude=exclude)\n",
    "\n",
    "    # if the data is so limited that a model could not be found, create a default model\n",
    "    if not best:\n",
    "        logging.warning('No best model found')\n",
    "        best = pycaret.classification.create_model('dt')\n",
    "\n",
    "    # tune the model\n",
    "    logging.info('Tuning the model...')\n",
    "    tuned_model = tune_model(best, verbose=False)\n",
    "    print(tuned_model)\n",
    "\n",
    "    # finalize the model and return the results\n",
    "    #logging.info('Finalizing the model...')\n",
    "    #return finalize_model(tuned_model)\n",
    "    \n",
    "    return tuned_model\n",
    "\n",
    "\n",
    "#create_model(\n",
    "#    data=X_train.join(class1),\n",
    "#    target='Level_1',\n",
    "#    session_id=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save model for level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and save model for level 1\n",
    "exclude_models = ['gbc', 'lightgbm'] # gbc and lightgbm are excluded for debugging as they take significantly longer than other models to fit and only offer a fairly small increase in accuracy.\n",
    "train_l1 = X_train.join(class1)\n",
    "\n",
    "level1_model = create_model(\n",
    "    data=train_l1,\n",
    "    target='Level_1',\n",
    "    session_id=23,\n",
    "    exclude=exclude_models)\n",
    "\n",
    "# dislay the model parameters\n",
    "print(level1_model)\n",
    "\n",
    "# display the model evaluation\n",
    "_ = predict_model(level1_model)\n",
    "\n",
    "# save the model\n",
    "finalize_model(level1_model)\n",
    "save_model(level1_model, 'models/level_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save models for level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level_values(data:pd.DataFrame, column:str):\n",
    "    '''\n",
    "    Get a list of distinct values in the specified column.\n",
    "    '''\n",
    "    return list(data[column].unique())\n",
    "\n",
    "#get_level_values(y_train, 'Level_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_level(data:pd.DataFrame, column:str, value:str):\n",
    "    '''\n",
    "    Filter the dataframe where the specified column matches the provided value.\n",
    "    '''\n",
    "    return data.query('%s == @value' % column)\n",
    "\n",
    "#filter_level(y_train, 'Level_1', '014303D1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_level_model(data:pd.DataFrame, features:pd.DataFrame, target:str, parent:str, parent_value:str, session_id:int=None, exclude=None, extra_features=None):\n",
    "    '''\n",
    "    Create a classification model for the dataset provided.\n",
    "\n",
    "    Parameters:\n",
    "        data: pandas.DataFrame\n",
    "            The DataFrame to build the model for.\n",
    "        features: pandas.DataFrame\n",
    "            The dateframe contraing the features used for predicting the targer variable.\n",
    "        target: str\n",
    "            The name of the target variable.\n",
    "        parent:str\n",
    "            The parent column name to filter the data on.\n",
    "        parent_value:str \n",
    "            The value to filter the parent column on.                       \n",
    "        exclude: list of str, default = None\n",
    "            To omit certain models from training and evaluation, pass a list containing model id in the exclude parameter. \n",
    "        session_id: int\n",
    "            The session id to use to control the model randomness.\n",
    "    Returns:\n",
    "        The model created for the level.\n",
    "    '''\n",
    "    # get the training dataset\n",
    "    df_train = filter_level(data, parent, parent_value) \n",
    "    # select only the target value and extra features\n",
    "    select_columns = [target] if extra_features is None else [target] + extra_features\n",
    "    df_train = df_train[select_columns] \n",
    "    # join the features to the training set\n",
    "    df_train = df_train.join(features, how='left') \n",
    "\n",
    "    # create the model\n",
    "    return create_model(\n",
    "        data=df_train,\n",
    "        target=target,\n",
    "        session_id=session_id,\n",
    "        exclude=exclude)\n",
    "\n",
    "#level_model = create_level_model(\n",
    "#    data=y_train, \n",
    "#    features=X_train,\n",
    "#    target='Level_2',\n",
    "#    session_id=23,\n",
    "#    exclude=['gbc', 'lightgbm'],\n",
    "#    parent='Level_1',\n",
    "#    parent_value='014303D1'\n",
    "#    )\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "\n",
    "#print(level_model)\n",
    "#_ = predict_model(level_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create One Model per parent level as per the coursework instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and save models for level 2\n",
    "# get the unique level 1 values\n",
    "parent_values = get_level_values(y_train, 'Level_1')\n",
    "\n",
    "for parent_value in tqdm(parent_values):\n",
    "    print('---------------- Building Model For: %s ----------------' % parent_value)\n",
    "\n",
    "    current_model = create_level_model(\n",
    "        data=y_train, \n",
    "        features=X_train,\n",
    "        target='Level_2',\n",
    "        session_id=23,\n",
    "        exclude=exclude_models,\n",
    "        parent='Level_1',\n",
    "        parent_value=parent_value,\n",
    "        extra_features=['Level_1']\n",
    "        )\n",
    "\n",
    "    # show the model created\n",
    "    print(current_model)\n",
    "    _ = predict_model(current_model)\n",
    "\n",
    "    # finalize and save the model\n",
    "    finalize_model(current_model)  \n",
    "    save_model(current_model, 'models/level_2/' + parent_value)\n",
    "\n",
    "IPython.display.clear_output() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with creating a single level 2 model\n",
    "\n",
    "If I was doing this task in practice I would not have started with a multi-model approach as it might be very inefficient for real-time scoring to load models in and out of memory with every prediction, considering that there can potentially be a large number of models in the recommended approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level2_model = create_model(\n",
    "    data=X_train.join(class2).join(class1),\n",
    "    target='Level_2',\n",
    "    session_id=23,\n",
    "    exclude=exclude_models)\n",
    "\n",
    "# dislay the model parameters\n",
    "print(level2_model)\n",
    "\n",
    "# display the model evaluation\n",
    "_ = predict_model(level2_model)\n",
    "finalize_model(level2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "# create a classification report for the single level 2 model\n",
    "df_predicted = predict_model(\n",
    "    level2_model, \n",
    "    data=X_test.join(y_test['Level_2'].astype(str)).join(y_test['Level_1'].astype(str)))\n",
    "\n",
    "print(classification_report(y_test['Level_2'].astype(str), df_predicted.Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save models for level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = create_level_model(\n",
    "    data=y_train, \n",
    "    features=X_train,\n",
    "    target='Level_3',\n",
    "    session_id=23,\n",
    "    exclude=['gbc', 'lightgbm'],\n",
    "    parent='Level_2',\n",
    "    #parent_value='0864A',\n",
    "    parent_value='7AED7',\n",
    "    extra_features=['Level_1', 'Level_2']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and save models for level 3\n",
    "parent_values = get_level_values(y_train, 'Level_2')\n",
    "\n",
    "for parent_value in tqdm(parent_values):\n",
    "    print('---------------- Building Model For: %s ----------------' % parent_value)\n",
    "\n",
    "    current_model = create_level_model(\n",
    "        data=y_train, \n",
    "        features=X_train,\n",
    "        target='Level_3',\n",
    "        session_id=23,\n",
    "        exclude=exclude_models + ['dummy'], # knn and the dummy model are causing a problem because some of the categories are too small\n",
    "        parent='Level_2',\n",
    "        parent_value=parent_value,\n",
    "        extra_features=['Level_1', 'Level_2']\n",
    "        )\n",
    "\n",
    "    # show the model created\n",
    "    print(current_model)\n",
    "    _ = predict_model(current_model)\n",
    "\n",
    "    # finalize and save the model\n",
    "    finalize_model(current_model)  \n",
    "    save_model(current_model, 'models/level_3/' + parent_value)\n",
    "\n",
    "IPython.display.clear_output() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the test set (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_level(data:pd.DataFrame, features:pd.DataFrame, target:str, parent:str):\n",
    "    # get the parent values to predict for\n",
    "    parent_values = get_level_values(data, parent)\n",
    "    predictions = pd.Series(name=target)\n",
    "\n",
    "    for parent_value in tqdm(parent_values):\n",
    "        # load the model\n",
    "        current_model = load_model('models/%s/%s' % (target, parent_value), verbose=False)\n",
    "\n",
    "        # get the dataset to perform the predictions on\n",
    "        current_data = filter_level(data, parent, parent_value)\n",
    "        current_data = current_data.join(features, how='left')\n",
    "\n",
    "        # perform the predictions\n",
    "        current_predictions = predict_model(current_model, data=current_data)\n",
    "        current_predictions = current_predictions.Label.rename(target)\n",
    "\n",
    "        # add the current predictions to the predictions list\n",
    "        predictions = pd.concat([predictions, current_predictions])\n",
    "\n",
    "    # join the predictions to the predicted dataframe\n",
    "    df_predicted = data.join(predictions)\n",
    "    return df_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Creating an empty Dataframe with column names only (depends on you how you do things)\n",
    "results = pd.DataFrame(columns=['Level1_Pred', 'Level2_Pred', 'Level3_Pred'])\n",
    "\n",
    "## Here we reload the saved models and use them to predict the levels\n",
    "# load model for level 1 (done for you)\n",
    "with open('level1.pk', 'rb') as nb:\n",
    "    model = pickle.load(nb)\n",
    "\n",
    "## loop through the test data, predict level 1, then based on that predict level 2\n",
    "## and based on level 2 predict level 3 (you need to load saved models accordingly)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the level 1 classes\n",
    "current_model = load_model('models/level_1', verbose=False)\n",
    "results = pd.DataFrame(predict_model(current_model, data=X_test).Label.rename('Level_1'))\n",
    "\n",
    "# perform the predictions for level 2\n",
    "results = predict_level(data=results, features=X_test, target='Level_2', parent='Level_1')\n",
    "\n",
    "# perform the predictions for level 3\n",
    "results = predict_level(data=results, features=X_test, target='Level_3', parent='Level_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After you add the predictions to the results dataframe\n",
    "## they should look like this\n",
    "results = results.add_suffix('_Pred')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Accuracy on each level (4 marks)\n",
    "Now you have the predictions for each level (in the test data), and you also have the actual levels, you can compute the accurcay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#create the evaluation dataframe\n",
    "df_evaluation = y_test.join(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 1 accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(data:pd.DataFrame, y_true, y_pred):\n",
    "    '''\n",
    "    Output the accuracy score for the specified level predictions.\n",
    "    '''\n",
    "    score = accuracy_score(\n",
    "        y_true=data[y_true].astype(str),\n",
    "        y_pred=data[y_pred].astype(str))\n",
    "\n",
    "    # output the score\n",
    "    print('%s Accuracy: %.2f' % (y_true, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the accuracy score\n",
    "print_accuracy(data=df_evaluation, y_true='Level_1', y_pred='Level_1_Pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 2 accuracy                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy(data=df_evaluation, y_true='Level_2', y_pred='Level_2_Pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 3 accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy(data=df_evaluation, y_true='Level_3', y_pred='Level_3_Pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Parent Evaluation\n",
    "\n",
    "The following evaluations are to test the accuracy of the level models by assuming the parent leven was correctly predicted.\n",
    "\n",
    "This information is useful to determine if a certain level is performing particularly badly and if the ngram, or feature count variations effects certain levels more than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level 1 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "current_model = load_model('models/level_1', verbose=False)\n",
    "df_predicted = pd.DataFrame(predict_model(current_model, data=X_test).Label.rename('Level_1'))\n",
    "\n",
    "# join the predictions predictions to the evaluation dataframe\n",
    "df_evaluation = y_test.join(df_predicted, rsuffix='_Pred')\n",
    "\n",
    "# output the model evaluation\n",
    "print(classification_report(df_evaluation['Level_1'].astype(str), df_evaluation['Level_1_Pred'].astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level 2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset for predicted values\n",
    "df_predicted = y_test.copy()\n",
    "df_predicted.drop(['Level_2', 'Level_3'], axis=1, inplace=True)\n",
    "df_predicted\n",
    "\n",
    "# perform the predictions for the specified level\n",
    "df_predicted = predict_level(data=df_predicted, features=X_test, target='Level_2', parent='Level_1')\n",
    "\n",
    "# join the predictions predictions to the evaluation dataframe\n",
    "df_evaluation = y_test.join(df_predicted, rsuffix='_Pred')\n",
    "\n",
    "# output the model evaluation\n",
    "print(classification_report(df_evaluation['Level_2'].astype(str), df_evaluation['Level_2_Pred'].astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level 3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset for predicted values\n",
    "df_predicted = y_test.copy()\n",
    "df_predicted.drop('Level_3', axis=1, inplace=True)\n",
    "\n",
    "# perform the predictions for the specified level\n",
    "df_predicted = predict_level(data=df_predicted, features=X_test, target='Level_3', parent='Level_2')\n",
    "\n",
    "# join the predictions predictions to the evaluation dataframe\n",
    "df_evaluation = y_test.join(df_predicted, rsuffix='_Pred')\n",
    "\n",
    "# output the model evaluation\n",
    "print(classification_report(df_evaluation['Level_3'].astype(str), df_evaluation['Level_3_Pred'].astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
